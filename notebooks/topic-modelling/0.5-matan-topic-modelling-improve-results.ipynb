{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:37:32.707336Z",
     "start_time": "2021-04-04T21:37:32.640516Z"
    }
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T00:04:23.662792Z",
     "start_time": "2021-04-05T00:04:23.563060Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from src.features import build_features\n",
    "\n",
    "import logging \n",
    "from pathlib import Path\n",
    "\n",
    "import scipy\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:37:40.196952Z",
     "start_time": "2021-04-04T21:37:37.890321Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import models\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from src.data.make_dataset import lematize\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "\n",
    "from src.data.DBConnection import DBConnection\n",
    "db = DBConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:37:40.292493Z",
     "start_time": "2021-04-04T21:37:40.199474Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\matan\\github\\linkedin-nlp\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:37:40.401616Z",
     "start_time": "2021-04-04T21:37:40.293491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\matan\\github\\linkedin-nlp\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "sql = \"SELECT details FROM positions;\"\n",
    "query = db.cur.execute(sql)\n",
    "documents = query.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T21:37:40.497013Z",
     "start_time": "2021-04-04T21:37:40.404195Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\matan\\github\\linkedin-nlp\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def process_documents(documents):\n",
    "    # try new preprocessing steps:\n",
    "    data = list(documents)\n",
    "    # remove \\n\n",
    "    data = [text[0].replace(\"\\n\", \" \") for text in data]\n",
    "    # remove https:// links \n",
    "    data = [re.sub(r\"((https{0,1}\\:/\\/\\w{3}\\S+)|(w{3}\\S+))\", \"\", text) for text in data]\n",
    "    # remove emails:\n",
    "    data = [re.sub(r\"(\\S+@\\S+)\", \"\", text) for text in data]\n",
    "    # remove phone numbers:\n",
    "    data = [re.sub(r\"([\\+\\s01-]*\\(*\\d{3}\\)*[-\\s]\\d{3}[-\\s]\\d{4})\", \"\", text) for text in data]\n",
    "    # remove uf0b7\n",
    "    data = [re.sub(r\"(\\uf0b7)\", \"\", text) for text in data]\n",
    "    # remove handles:\n",
    "    data = [re.sub(r\"(\\@\\S+)\", \"\", text) for text in data]\n",
    "    # remove french documents\n",
    "    data = [doc for doc in data if \"une\" not in doc and \"connaissance\" not in doc]\n",
    "    return data\n",
    "\n",
    "def docs_to_words(documents):\n",
    "    for doc in documents:\n",
    "        yield(gensim.utils.simple_preprocess(str(doc), deacc=True))  # deacc=True removes punctuations\n",
    "        \n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T02:19:24.058985Z",
     "start_time": "2021-04-05T02:19:21.517474Z"
    }
   },
   "outputs": [],
   "source": [
    "# get stop words and add some super-common words amoung the documents\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"yelp\", \"agoda\"])\n",
    "\n",
    "\n",
    "# clean all the documents (i.e., use regex to remove emails, urls, phone numbers, french documents, etc.)\n",
    "clean_docs = process_documents(documents)\n",
    "\n",
    "# use simple_preprocess to remove punctuation:\n",
    "data_words = list(docs_to_words(clean_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-05T02:19:21.807Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=35) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[processed_corpus], threshold=20)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(processed_corpus)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Form trigrams:\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams)\n",
    "\n",
    "# Lematize docs:\n",
    "data_lemmatized = [lematize(words).split(\" \") for words in data_words_trigrams]\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Bag of words:\n",
    "print('Total Vocabulary Size Pre Filter:', len(id2word))\n",
    "id2word.filter_extremes(no_below=5, no_above=0.7)\n",
    "print('Total Vocabulary Size Post Filter:', len(id2word))\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# TF-IDF matrix:\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-05T02:19:22.313Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# determine optimal number of topics using coherence score:\n",
    "coherence_vals = []\n",
    "model_list =[]\n",
    "start = 2\n",
    "stop = 10\n",
    "step = 1\n",
    "\n",
    "for num_topics in range(start, stop, step):\n",
    "    # make LDa model and calc coherence:\n",
    "    model = LdaModel(corpus=tfidf_corpus, id2word=id2word, num_topics=num_topics, passes=5, iterations=100)\n",
    "    model_list.append(model)\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=id2word, coherence='c_v', processes=4)\n",
    "    coherence_vals.append(coherence_model.get_coherence())\n",
    "    \n",
    "x = range(start, stop, step)\n",
    "plt.plot(x, coherence_vals)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-05T02:19:22.890Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = model_list[coherence_vals.index(max(coherence_vals))]\n",
    "print(f\"Best model coherence score is {max(coherence_vals):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-05T02:19:23.245Z"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(best_model, tfidf_corpus, id2word, sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-05T02:19:23.974Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display topics:\n",
    "topics = best_model.print_topics(num_words=10)\n",
    "topics_df = pd.DataFrame(topics)\n",
    "topics_df = topics_df[1].str.replace(\"\\\\s\\\\+\", \"\", regex=True).str.replace('\\\\\"', \"\").str.split(\" \", expand=True).T\n",
    "topics_df.columns = [f\"topic_{t+1}\" for t in topics_df.columns]\n",
    "\n",
    "doc_topics = np.argmax(best_model.get_topics(), axis=0)\n",
    "plt.bar(height=np.bincount(doc_topics), x=list(range(1, best_model.num_topics+1)));\n",
    "    \n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using TF-IDF versus just TF increased coherence score and seemed to increase interpretability in topics\n",
    "- Using bi- and tr- grams improved coherence score, as well as tuning the threshold parameters\n",
    "- Increasing LDA passes > 4 increases coherence and interpretability, but passes = 10 seems to overfit the data and the topics end up being specific to companies (i.e., topic words are bell, microsoft, rogers, etc.\n",
    "- Coherence score cannot be relied on completely, sometimes the topics are not great, or the best model may not include a topic I know is there (i.e., Civil Engineering, which I purposely put in, around 11% of total dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved model coherence score and subjective results by using TF-IDF, decreasing the bi- and tri-gram model parameters, increasing passes and iterations of LDA model. LDA model seems to be able to tease out the major topics, which seem possible:<br>\n",
    "1. Accountant\n",
    "2. Food/Restaurant Work\n",
    "3. Management Consulting (Deloitte?)\n",
    "4. Health Related Research (this may be a slightly unstable category, probably related to Data Scientist)\n",
    "5. Geotechnical/Civil Engineer\n",
    "6. Data Scientist\n",
    "7. Civil Engineer\n",
    "<br>\n",
    "It appears that this model was reasonably successful, there are some clear topics but at the same time it would appear that some of the topics overlap.\n",
    "\n",
    "<br><br>\n",
    "Further Research<br>\n",
    "Look into whether the job title matches the topics produced using the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
